{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f255478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import h5py\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import library as lib\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "from sklearn.manifold import trustworthiness\n",
    "\n",
    "import seaborn as sns\n",
    "#from sklearn.metrics import silhouette_score\n",
    "from itertools import groupby\n",
    "import string\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#import multiprocess as mp\n",
    "from ete3 import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed87e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592f639",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "work_dir = \"/c/Users/alex/FortgeschrittenesPraktikum/PyTorch_Lightning_VAE\"\n",
    "inputs = \"./inputs/\"\n",
    "out_path = \"./out_activ_fct/sig/\"\n",
    "\n",
    "writer = False\n",
    "gpu = False\n",
    "\n",
    "activation_function = \"sigmoid\"\n",
    "#exec(f\"torch.nn.functional.{get_activation(self)}({self.latent(self.encoder(x))})\")\n",
    "\n",
    "metric_list = [\"cosine\",\"euclidean\", \"manhattan\", \"ts_ss\"]\n",
    "metric_list = metric_list\n",
    "print(\"metric: \",metric_list)\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "dim = 2560\n",
    "dim3 = int(dim * 1/16)\n",
    "dim2 = int(dim * 1/8)\n",
    "dim1 = int(dim * 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc19381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(exclude, \"r\") as ex:\n",
    "#    reader = csv.reader(ex, delimiter='\\t')\n",
    "#    column1 = [row[0] for row in reader]\n",
    "#print(column1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4a98f",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74668b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make numpy file with (seq header, embedding)\n",
    "#esm2 = h5py.File(inputs+\"KLK_esm2_reduced.h5\", 'r')\n",
    "esm2 = h5py.File(\"chumby_data/phosphatase_esm2_3b.h5\", 'r')\n",
    "list(esm2.keys())\n",
    "# List all groups\n",
    "keys = list(esm2.keys())\n",
    "#print(keys) \n",
    "# Get the data\n",
    "embedding = []\n",
    "headers = []\n",
    "for key in keys:\n",
    "    #data\n",
    "    emb = esm2[key][:]\n",
    "    #headers\n",
    "    #replacing mistakes\n",
    "    key = key.replace(\"isoform=\", \"isoform_\").replace(\"=\", \"\").replace(\":\", \"_\")\n",
    "    if key == \"GZMA_Canis_lupus\":\n",
    "        print(\"header is now changed\")\n",
    "        key = \"GZMA_Canis_lupus_1\"\n",
    "    #exclude\n",
    "    #if key not in column1:\n",
    "    headers.append(key)\n",
    "    embedding.append(emb)\n",
    "\n",
    "headers = np.array(headers)\n",
    "embedding = np.array(embedding)\n",
    "\n",
    "#split train test\n",
    "headers_first, headers_test, embedding_first, embedding_test = train_test_split(headers, embedding, test_size=0.1, random_state=42)\n",
    "#split train validation\n",
    "headers_train, headers_validate, embedding_train, embedding_validate = train_test_split(headers_first, embedding_first, test_size =0.2, random_state=42)\n",
    "print(\"total:\", headers.shape)\n",
    "print(\"training+validation:\", headers_first.shape)\n",
    "print(\"test:\", headers_test.shape)\n",
    "print(\"training:\", headers_train.shape)\n",
    "print(\"validation:\", headers_validate.shape)\n",
    "print(len(headers_train), len(embedding_train))\n",
    "print(len(headers_validate), len(embedding_validate))\n",
    "\n",
    "#training_df = pd.DataFrame({'header': headers_train, 'embedding' : embedding_train})\n",
    "#validate_df = pd.DataFrame({'header': headers_validate, 'embedding' : embedding_validate})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1f1d6",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d14fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lib.makedataset(headers_train, embedding_train)\n",
    "val_dataset = lib.makedataset(headers_validate, embedding_validate)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52afc8f",
   "metadata": {},
   "source": [
    "# Training VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396aa8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if prott5 used: dtype=float16 -> error with .linear()\n",
    "\n",
    "#encoder_layers, latent_dim, decoder_layers\n",
    "\n",
    "encoder_layers = [dim, dim1, dim2, dim3]\n",
    "latent_dim = dim3\n",
    "decoder_layers = [dim3, dim2, dim1, dim]\n",
    "\n",
    "# Initialize the VAE model\n",
    "vae = lib.VariationalAutoencoder(encoder_layers, latent_dim, decoder_layers)\n",
    "\n",
    "if gpu:\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", devices = 1, max_epochs=epoch, log_every_n_steps=1, check_val_every_n_epoch=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(accelerator=\"auto\", max_epochs=epoch, log_every_n_steps=1, check_val_every_n_epoch=1)\n",
    "\n",
    "# Train the VAE model\n",
    "trainer.fit(vae, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb159b5",
   "metadata": {},
   "source": [
    "# Fix Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_number(root_dir):\n",
    "    largest_number = 0\n",
    "    print(root_dir)\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for dir_name in dirs:\n",
    "            print(dir_name)\n",
    "            match = re.search(r'\\d+', dir_name)\n",
    "            if match:\n",
    "                current_number = int(match.group())\n",
    "                if current_number > largest_number:\n",
    "                    largest_number = current_number\n",
    "    return largest_number\n",
    "\n",
    "largest_num = find_largest_number(work_dir+\"/lightning_logs\")\n",
    "#hard coded cause its broken\n",
    "largest_num = 1\n",
    "\n",
    "#Fix formatting\n",
    "#with open(work_dir+'/lightning_logs/version_' + str(largest_num) + '/metrics.csv', 'r') as file:\n",
    "with open(\"/c/Users/alex/FortgeschrittenesPraktikum/PyTorch_Lightning_VAE/lightning_logs/version_1/metrics.csv\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "combined_rows = []\n",
    "header_row = ['train_loss','reconstruction_loss','kl_loss','epoch','step','validation_loss']\n",
    "\n",
    "combined_rows.append(header_row)\n",
    "for i in range(1, len(rows) -1, 2):\n",
    "    train_loss = rows[i][0]\n",
    "    reconstruction_loss = rows[i][1]\n",
    "    kl_loss = rows[i][2]\n",
    "    epoch = rows[i+1][3]\n",
    "    step = rows[i+1][4]\n",
    "    validation_loss = rows[i+1][5]\n",
    "    combined_rows.append([train_loss,reconstruction_loss,kl_loss,epoch,step,validation_loss])\n",
    "\n",
    "with open(work_dir+'lightning_logs/version_' + str(largest_num) + '/new_metrics.csv', 'w', newline = \"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(combined_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06a7c9",
   "metadata": {},
   "source": [
    "# Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "# read csv file\n",
    "df = pd.read_csv('lightning_logs/version_' + str(largest_num) + '/new_metrics.csv')\n",
    "\n",
    "df1 = df[df['epoch'] >= int(epoch)/2]\n",
    "\n",
    "# plot 0\n",
    "plt.plot('epoch', 'train_loss', data=df)\n",
    "plt.plot('epoch', 'reconstruction_loss', data=df)\n",
    "plt.plot('epoch', 'validation_loss', data=df)\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['loss', 'reconstruction loss', 'validation_loss'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot 1\n",
    "plt.plot('epoch', 'train_loss', data=df1)\n",
    "plt.plot('epoch', 'reconstruction_loss', data=df1)\n",
    "plt.plot('epoch', 'validation_loss', data=df1)\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['loss', 'reconstruction loss', 'validation_loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80f697",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b54263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encode = vae.encoder(torch.Tensor(embedding))\n",
    "print(encode)\n",
    "\n",
    "embedding_out = encode.detach() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7242a3",
   "metadata": {},
   "source": [
    "# Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1944fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = int(epoch) +1\n",
    "print(\"epoch: \",epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8da888",
   "metadata": {},
   "source": [
    "# Neighbor joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighbor_joining and create tree\n",
    "def nj(metric_list, emb, headers, writer, out_path, epoch, trained, activation_function,title):\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    PLOTS_PER_ROW = 2\n",
    "    \n",
    "    #fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))\n",
    "    #plt.subplots_adjust(hspace=0.5)\n",
    "    #fig.suptitle('Distance Matrix from '+str(epoch)+' Training '+title, fontsize=18, y=0.95)\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(metric)\n",
    "\n",
    "        out_newick = out_path+\"KLK_\"+activation_function+\"_esm2_nj_\"+metric+\"_\"+trained+\".newick\"\n",
    "        print(out_newick)\n",
    "\n",
    "\n",
    "        #distance metric\n",
    "        dist = lib.distance_metric(emb)\n",
    "        distmat = dist.get_metric(emb, metric)\n",
    "        \n",
    "        #print(np.where(distmat > 150))\n",
    "\n",
    "        \n",
    "        #neighbor joining to tree\n",
    "        nj = lib.neighbor_joining(distmat,headers)\n",
    "        newick = nj.get_newick(distmat,headers)\n",
    "\n",
    "        #trustworthiness\n",
    "        _distmat  = lib.cophenetic_distmat(newick, names=headers)\n",
    "        _trustworthiness = trustworthiness(distmat, _distmat, n_neighbors=10, metric='precomputed')\n",
    "        print(\"trustworthiness: \", _trustworthiness)\n",
    "\n",
    "\n",
    "        #silhouette = silhouette_score(_distmat, headers, metric='precomputed')\n",
    "        #silhouette = lib.silhouette.get_silhouette(distmat, headers)\n",
    "        #print(\"silhouette: \", silhouette)\n",
    "\n",
    "        sns.heatmap(data= distmat,cmap = sns.cm.rocket_r).set(title=metric.capitalize()+' Distance Matrix from '+str(epoch)+' Training '+title)\n",
    "        #sns.heatmap(ax=axes[i, j], data= distmat,cmap = sns.cm.rocket_r).set(title=metric.capitalize()+' Distance Matrix')\n",
    "        plt.savefig(out_path+'heatmap_'+activation_function+'_'+metric+'_'+trained+'.png', dpi=400)\n",
    "        \n",
    "        j+=1\n",
    "        if j%PLOTS_PER_ROW==0:\n",
    "            i+=1\n",
    "            j=0\n",
    "            \n",
    "        if writer:\n",
    "            with open(out_newick, 'w') as w:\n",
    "                w.write(newick)\n",
    "                w.close()\n",
    "                                \n",
    "    #plt.show()\n",
    "    #plt.savefig(out_path+'heatmap_'+activation_function+'_summary_'+trained+'.png', dpi=400)\n",
    "    \n",
    "print(\"epoch: \",epoch)\n",
    "nj(metric_list, embedding_out,headers, writer, out_path, str(epoch)+\" Epoch\", str(epoch),activation_function, '('+activation_function.capitalize()+')')            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2121c8",
   "metadata": {},
   "source": [
    "# Reference NJ: no training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nj(metric_list, embedding, headers, False, out_path, \"No\", \"notraining\",activation_function, \"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3372da",
   "metadata": {},
   "source": [
    "# UPGMA and create tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9349fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgma(metric_list, emb, headers, writer, out_path, epoch, trained ,activation_function):\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(\"metric:\", metric)\n",
    "        \n",
    "        out_newick = out_path+\"KLK_\"+activation_function+\"_esm2_upgma_\"+metric+\"_\"+trained+\".newick\"\n",
    "\n",
    "        #distance metric\n",
    "        dist = lib.distance_metric(emb)\n",
    "        distmat = dist.get_metric(emb, metric)\n",
    "        #print(distmat)\n",
    "\n",
    "        #upgma to tree\n",
    "        upgma = lib.upgma(distmat,headers)\n",
    "\n",
    "        #trustworthiness\n",
    "        _distmat  = lib.cophenetic_distmat(upgma, names=headers)\n",
    "        _trustworthiness = trustworthiness(distmat, _distmat, n_neighbors=10, metric='precomputed')\n",
    "        print(\"trustworthiness: \", _trustworthiness)\n",
    "\n",
    "        \n",
    "        #sns.heatmap(distmat,cmap = sns.cm.rocket_r).set(title=metric.capitalize()+' Distance Matrix from '+epoch+' Training')\n",
    "        #plt.savefig(out_path+'heatmap_'+activation_function+'_'+metric+'_'+trained+'.png', dpi=400)\n",
    "        \n",
    "        if writer:\n",
    "            with open(out_newick, 'w') as w:\n",
    "                w.write(upgma)\n",
    "                w.close()\n",
    "\n",
    "\n",
    "print(\"epoch: \",epoch)\n",
    "upgma(metric_list, embedding_out, headers, writer, out_path, str(epoch)+\" Epochs\", str(epoch),activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18551bef",
   "metadata": {},
   "source": [
    "# Reference UPGMA: no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upgma(metric_list, embedding, headers, False, out_path, \"No\",\"notraining\",activation_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c287ad",
   "metadata": {},
   "source": [
    "# T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_df = pd.read_csv(inputs+\"KLK_iTOL.txt\", sep='\\t',names=('protein', 'range', 'colour', 'family'))\n",
    "colour_df = colour_df.drop([0,1,2])\n",
    "\n",
    "colour_df['family'] = colour_df['family'].replace(['KLK4','KLK5','KLK7'], 'KLK4/5/7',regex = False)\n",
    "colour_df['family'] = colour_df['family'].replace(['KLK8','KLK10','KLK12'], 'KLK8/10/12',regex = False)\n",
    "colour_df['family'] = colour_df['family'].replace(['KLK1','Mammalian KLK1','non-mammalian KLK1'], 'KLK1',regex = False)\n",
    "colour_df['family'] = colour_df['family'].replace(['KLK1','KLK2','Reptilian KLKs'], 'KLK1/2/Reptilian KLK',regex = False)\n",
    "colour_df['family'] = colour_df['family'].replace(['KLK4/5/7','KLK8/10/12'], 'KLK4/5/7/8/10/12',regex = False)\n",
    "\n",
    "types = len(colour_df[\"family\"].unique())\n",
    "print(types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b939226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(embeddings, df_protein_types, title_name, dataset,n_types):\n",
    "    \n",
    "    tsne = TSNE(perplexity = 22, learning_rate = 100, n_iter = 1000, random_state= 42)#(n_components=2, verbose=1, random_state=123)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "    df_subset = pd.DataFrame()\n",
    "    df_subset[\"Type\"] = df_protein_types[\"family\"]\n",
    "    df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "    df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "\n",
    "    #print(df_subset)\n",
    "    sns.scatterplot(\n",
    "        x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "        hue=\"Type\",\n",
    "        palette=sns.color_palette(\"tab20\",n_types),\n",
    "        edgecolor = 'white',\n",
    "        data=df_subset,\n",
    "        legend=\"full\"\n",
    "    ).set(title= title_name)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1),loc='upper left', borderaxespad=0,fontsize=\"6\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path+'tsne_'+dataset+'.png', dpi=600)\n",
    "    \n",
    "    return tsne_results\n",
    "\n",
    "\n",
    "tsne_results_trained = tsne(embedding_out,colour_df, f\"T-SNE of VAE Embeddings ({activation_function.capitalize()})\", f\"{activation_function}_trained\",types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00334bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results_ref = tsne(embedding,colour_df, \"T-SNE of Embeddings Without Training\",\"reference\",types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne_results_trained.shape)\n",
    "nj(metric_list, tsne_results_ref, headers, writer, out_path, \"T-SNE\", \"tsne\",\"\",\"\")\n",
    "#upgma(metric_list,  tsne_results_ref, headers, writer, out_path, str(epoch)+\" Epoch T-SNE\", \"tsne\", activation_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921b82c",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6820d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(embeddings, df_protein_types, title_name, dataset,n_types):\n",
    "    \n",
    "    pca = PCA(n_components=2,random_state=42)\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"Type\"] = colour_df[\"family\"]\n",
    "    df['pca-one'] = pca_result[:,0]\n",
    "    df['pca-two'] = pca_result[:,1] \n",
    "    #df['pca-three'] = pca_result[:,2]\n",
    "    print()\n",
    "\n",
    "    print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(14,10))\n",
    "    sns.scatterplot(\n",
    "        x=\"pca-one\", y=\"pca-two\",\n",
    "        hue=\"Type\",\n",
    "        palette=sns.color_palette(\"tab20\", n_types),\n",
    "        data=df,\n",
    "        legend=\"full\").set(title= title_name)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path+'pca_'+dataset+'.png', dpi=600)\n",
    "    \n",
    "    return pca_result\n",
    "\n",
    "\n",
    "\n",
    "pca_results_trained = pca(embedding_out, colour_df, f\"PCA of VAE Embeddings ({activation_function.capitalize()})\", f\"{activation_function}_trained\",types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ffbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_reference = pca(embedding, colour_df,  f\"PCA of Embeddings Without Training\", \"reference\",types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55707cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_results_trained.shape)\n",
    "#nj(metric_list, pca_results_reference, headers, writer, out_path, \"PCA\", \"pca\", activation_function)\n",
    "upgma(metric_list,  pca_results_reference, headers, writer, out_path, \"PCA\", \"pca\", activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e2978",
   "metadata": {},
   "source": [
    "# Branch Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nj_fast(metric, emb, headers):\n",
    "\n",
    "    #distance metric\n",
    "    dist = lib.distance_metric(emb)\n",
    "    distmat = dist.get_metric(emb, metric)\n",
    "\n",
    "    #neighbor joining to tree\n",
    "    nj = lib.neighbor_joining(distmat,headers)\n",
    "    newick = nj.get_newick(distmat,headers)\n",
    "\n",
    "    return newick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(n_samples, emb):\n",
    "    sample_list = []\n",
    "    \n",
    "    while n_samples != 0:\n",
    "        encode = vae.encoder(torch.Tensor(emb))\n",
    "        embedding_out = encode.detach() \n",
    "        print(embedding_out)\n",
    "        \n",
    "        sample_list.append(embedding_out)\n",
    "        \n",
    "        n_samples = n_samples -1\n",
    "        \n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ab936",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "THREADS = 6\n",
    "\n",
    "samples = get_samples(n_samples, embedding)\n",
    "\n",
    "with mp.Pool(THREADS) as pool:\n",
    "    args = [(\"manhattan\", i, headers) for i in samples]\n",
    "    #print(len(args))\n",
    "\n",
    "    output = pool.starmap(get_nj_fast, args)\n",
    "    #print(output[0])\n",
    "\n",
    "    \n",
    "#with open(newick_replicates, 'w') as w:\n",
    "#    w.write('\\n'.join(output))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe41b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "newick_reference = \"./out_activ_fct/sigmoid/KLK_sigmoid_esm2_nj_manhattan_1.newick\"\n",
    "\n",
    "reference_tree = Tree(newick_reference)\n",
    "reference_bits, reference_nodes = lib.get_bipartitions(reference_tree, headers)\n",
    "print(\"total: \",len(headers))\n",
    "print(\"after bi-partition: \",len(reference_bits))\n",
    "#print(reference_bits)\n",
    "\n",
    "#tree_list = [Tree(i) for i in output]\n",
    "#print(len(tree_list))\n",
    "#replicate_trees  = ((reference_bits, i, headers) for i in map(Tree, output))\n",
    "#for tree in replicate_trees:\n",
    "#    print(tree)\n",
    "\"\"\"\n",
    "with mp.Pool(THREADS) as pool:\n",
    "    args = [(reference_bits, i, headers) for i in output]\n",
    "    #print(args[0])\n",
    "    support_bits = pool.starmap(lib.get_support, args)\n",
    "    support = (100 * np.array(support_bits).mean(0)).astype(int)\n",
    "    #print(support)\n",
    "\n",
    "\n",
    "for percentage, node in zip(support, reference_nodes):\n",
    "    node.support = percentage\n",
    "    #print(percentage)\n",
    "    #print(node.support)\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "\n",
    "for node in reference_tree.traverse():\n",
    "    total = total+1\n",
    "    if node not in reference_nodes:\n",
    "        node.support = 0\n",
    "        count = count+1\n",
    "        \n",
    "print(total)\n",
    "print(count)\n",
    "\n",
    "with open(out_path+\"branch_support.newick\", 'w') as w:\n",
    "    w.write(reference_tree.write())\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FP_Lightning",
   "language": "python",
   "name": "fp_lightning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
